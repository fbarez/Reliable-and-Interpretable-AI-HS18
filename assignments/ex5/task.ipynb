{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from torchvision import datasets, transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "from model import Net, ConvNet\n",
    "from lbfgsb import *\n",
    "device = torch.device(\"cpu\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "# note that this time we do not perfrom the normalization operation, see next cell\n",
    "test_dataset = datasets.MNIST('mnist_data/', train=False, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return (x - 0.1307)/0.3081\n",
    "\n",
    "# we load the body of the neural net trained last time...\n",
    "NN1_logits = torch.load('model_conv.net', map_location='cpu') \n",
    "NN2_logits = torch.load('model_ff.net', map_location='cpu') \n",
    "\n",
    "# ... and add the data normalization as a first \"layer\" to the network\n",
    "# this allows us to search for adverserial examples to the real image, rather than\n",
    "# to the normalized image\n",
    "NN1_logits = nn.Sequential(Normalize(), NN1_logits)\n",
    "NN2_logits = nn.Sequential(Normalize(), NN2_logits)\n",
    "\n",
    "# and here we also create a version of the model that outputs the class probabilities\n",
    "NN1 = nn.Sequential(NN1_logits, nn.Softmax())\n",
    "NN2 = nn.Sequential(NN2_logits, nn.Softmax())\n",
    "\n",
    "# we put the neural net into evaluation mode (this disables features like dropout)\n",
    "NN1_logits.eval()\n",
    "NN2_logits.eval()\n",
    "NN1.eval()\n",
    "NN2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a show function for later\n",
    "def show(result_gd, result_gd_bounds, result_lbfgsb):    \n",
    "    gd_s, gd_l, gd_i, gd_t, gd_n = result_gd\n",
    "    gdb_s, gdb_l, gdb_i, gdb_t, gdb_n = result_gd_bounds\n",
    "    lbfgsb_s, lbfgsb_l, lbfgsb_i, lbfgsb_t = result_lbfgsb\n",
    "\n",
    "    def print_res(title, solved, loss, i, time, it=None):\n",
    "        print(title + ':')\n",
    "        print('\\tSolved:', solved)\n",
    "        print('\\tLoss:', loss)\n",
    "        print('\\tTime:', time, 's')        \n",
    "        if it is not None:\n",
    "            print('\\tGradient Descent iterations:', it)\n",
    "        p1 = NN1(torch.from_numpy(i).reshape((1, 1, 28, 28))).detach().numpy()\n",
    "        p2 = NN2(torch.from_numpy(i).reshape((1, 1, 28, 28))).detach().numpy()\n",
    "        print('\\tNN1_logits class: {} (p = {:.2f}) '.format(p1.argmax(), p1.max()))\n",
    "        print('\\tNN2_logits class: {} (p = {:.2f}) '.format(p2.argmax(), p2.max()))            \n",
    "\n",
    "    print_res('Gradient Descent', gd_s, gd_l, gd_i, gd_t, gd_n)\n",
    "    print_res('Gradient Descent w. Bounds', gdb_s, gdb_l, gdb_i, gdb_t, gdb_n)\n",
    "    print_res('L-BFGS-B', lbfgsb_s, lbfgsb_l, lbfgsb_i, lbfgsb_t)    \n",
    "    \n",
    "    f, axarr = plt.subplots(1,3, figsize=(18, 16))\n",
    "    axarr[0].imshow(gd_i.reshape(28, 28), cmap='gray')\n",
    "    axarr[0].set_title('Gradient Descent')\n",
    "    axarr[1].imshow(gdb_i.reshape(28, 28), cmap='gray')\n",
    "    axarr[1].set_title('Gradient Descent w. Bounds')\n",
    "    axarr[2].imshow(lbfgsb_i.reshape(28, 28), cmap='gray')\n",
    "    axarr[2].set_title('L-BFGS-B')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nine = test_dataset[12][0]\n",
    "fig = plt.figure()\n",
    "plt.imshow(nine.numpy().reshape((28,28)), cmap='gray')\n",
    "plt.title('Original Nine')\n",
    "None # avoid printing title object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints:\n",
    "\n",
    "- You are given a code skeleton with a few functions, but you will probably need to add some more arugments to the functions and their calls.\n",
    "- Split the target \"variable\" i into a fixed part (where it is equal to nine) and a variable part (that is optimized; set requires_grad for this one). Both parts should be tensors and can be combined into another tensor representing i.\n",
    "- Create these two tensors once and then have a function that combines them and calculates the loss.\n",
    "- For the loss it is easiest to implement a function implements the loss translation for the less-or-equal ($\\leq$) and less ($<$) operators from the lecutre. You can express all parts of the loss function with this. Make this parametric in the choice of d.\n",
    "- If implemented correctly your code should not run more than a few seconds.\n",
    "- There is no L-BFGS-B optimizer for pytroch yet. We provide a function that uses scipy to do this instead (see below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_gd(max_iter=100, **kwargs):\n",
    "    t0 = time.time()\n",
    "    t1 = time.time()\n",
    "    loss = 0\n",
    "    solved = False\n",
    "    \n",
    "    #Hint:\n",
    "    # Use pytroch SGD optimizer.\n",
    "    # Even though it says Stochastic Gradient Descent, we will perfrom Gradient Descent.\n",
    "    \n",
    "    #return:\n",
    "    #solved: Bool; did you find a solution\n",
    "    #loss: Float; value of loss at the end    \n",
    "    #i: numpy array; the resulting i\n",
    "    #t: float; how long the execution took\n",
    "    #nr: number of iterations\n",
    "    return solved, loss, nine.detach().numpy(), t1 - t0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feel free to add args to this function\n",
    "def solve_lbfgsb(**kwargs):\n",
    "    t0 = time.time()\n",
    "    t1 = time.time()\n",
    "    loss = 0\n",
    "    solved = False\n",
    "\n",
    "    #Hint:\n",
    "    # Use the provided lbfgsb(var, min_val, max_val, loss_fn, zero_grad_fn)\n",
    "    # function.\n",
    "    # It takes the tensor to optimize (var), the min and max value for each entry (a scalar),\n",
    "    # a function that returns the current loss-tensor and a function that sets the \n",
    "    # gradients of everything used (NN1_logits, NN2_logits) and i_var to zero.\n",
    "    # This funciton does not return anything but changes var.\n",
    "\n",
    "    \n",
    "    #return:\n",
    "    #solved: Bool; did you find a solution\n",
    "    #loss: Float; value of loss at the end    \n",
    "    #i: numpy array; the resulting i\n",
    "    #t: float; how long the execution took\n",
    "    return solved, loss, nine.detach().numpy(), t1 - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using logits, initialized with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(solve_gd(init_zero=True),\n",
    "     solve_gd(add_bounds=True, init_zero=True),\n",
    "     solve_lbfgsb(init_zero=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first image has weird colors as not all values are in [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using logits, initialized with original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(solve_gd(),\n",
    "     solve_gd(add_bounds=True),\n",
    "     solve_lbfgsb())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using probabilites, initialized with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(solve_gd(use_logits=False, init_zero=True),\n",
    "     solve_gd(use_logits=False, init_zero=True),\n",
    "     solve_lbfgsb(use_logits=False, init_zero=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using probabilites, initialized with original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(solve_gd(use_prob=True),\n",
    "     solve_gd(add_bounds=True, use_prob=True),\n",
    "     solve_lbfgsb(use_prob=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that using probabilities is not a viable approach. The numerical optimization problem becomes basically intractable due to the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## different box constraint (task 1.7; optional), using logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(solve_gd(box=2),\n",
    "     solve_gd(add_bounds=True, box=2),\n",
    "     solve_lbfgsb(box=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the region covered by box 2 is mostly emty it does not matter much wether we use init_zero or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
