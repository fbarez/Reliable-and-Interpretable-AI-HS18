{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.core.debugger import set_trace\n",
    "from torchvision import datasets, transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "from model import Net, ConvNet\n",
    "from lbfgsb import *\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb79a28cd30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "# note that this time we do not perfrom the normalization operation, see next cell\n",
    "test_dataset = datasets.MNIST('mnist_data/', train=False, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manu/git/riai18/assignments/ex5/.env/lib/python3.6/site-packages/torch/serialization.py:425: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/manu/git/riai18/assignments/ex5/.env/lib/python3.6/site-packages/torch/serialization.py:425: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/manu/git/riai18/assignments/ex5/.env/lib/python3.6/site-packages/torch/serialization.py:425: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Normalize()\n",
       "    (1): Net(\n",
       "      (fc): Linear(in_features=784, out_features=200, bias=True)\n",
       "      (fc2): Linear(in_features=200, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (1): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Normalize(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return (x - 0.1307)/0.3081\n",
    "\n",
    "# we load the body of the neural net trained last time...\n",
    "NN1_logits = torch.load('model_conv.net', map_location='cpu') \n",
    "NN2_logits = torch.load('model_ff.net', map_location='cpu') \n",
    "\n",
    "# ... and add the data normalization as a first \"layer\" to the network\n",
    "# this allows us to search for adverserial examples to the real image, rather than\n",
    "# to the normalized image\n",
    "NN1_logits = nn.Sequential(Normalize(), NN1_logits)\n",
    "NN2_logits = nn.Sequential(Normalize(), NN2_logits)\n",
    "\n",
    "# and here we also create a version of the model that outputs the class probabilities\n",
    "NN1 = nn.Sequential(NN1_logits, nn.Softmax())\n",
    "NN2 = nn.Sequential(NN2_logits, nn.Softmax())\n",
    "\n",
    "# we put the neural net into evaluation mode (this disables features like dropout)\n",
    "NN1_logits.eval()\n",
    "NN2_logits.eval()\n",
    "NN1.eval()\n",
    "NN2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a show function for later\n",
    "def show(result_gd, result_gd_bounds, result_lbfgsb):    \n",
    "    gd_s, gd_l, gd_i, gd_t, gd_n = result_gd\n",
    "    gdb_s, gdb_l, gdb_i, gdb_t, gdb_n = result_gd_bounds\n",
    "    lbfgsb_s, lbfgsb_l, lbfgsb_i, lbfgsb_t = result_lbfgsb\n",
    "\n",
    "    def print_res(title, solved, loss, i, time, it=None):\n",
    "        print(title + ':')\n",
    "        print('\\tSolved:', solved)\n",
    "        print('\\tLoss:', loss)\n",
    "        print('\\tTime:', time, 's')        \n",
    "        if it is not None:\n",
    "            print('\\tGradient Descent iterations:', it)\n",
    "        p1 = NN1(torch.from_numpy(i).reshape((1, 1, 28, 28))).detach().numpy()\n",
    "        p2 = NN2(torch.from_numpy(i).reshape((1, 1, 28, 28))).detach().numpy()\n",
    "        print('\\tNN1_logits class: {} (p = {:.2f}) '.format(p1.argmax(), p1.max()))\n",
    "        print('\\tNN2_logits class: {} (p = {:.2f}) '.format(p2.argmax(), p2.max()))            \n",
    "\n",
    "    print_res('Gradient Descent', gd_s, gd_l, gd_i, gd_t, gd_n)\n",
    "    print_res('Gradient Descent w. Bounds', gdb_s, gdb_l, gdb_i, gdb_t, gdb_n)\n",
    "    print_res('L-BFGS-B', lbfgsb_s, lbfgsb_l, lbfgsb_i, lbfgsb_t)    \n",
    "    \n",
    "    f, axarr = plt.subplots(1,3, figsize=(18, 16))\n",
    "    axarr[0].imshow(gd_i.reshape(28, 28), cmap='gray')\n",
    "    axarr[0].set_title('Gradient Descent')\n",
    "    axarr[1].imshow(gdb_i.reshape(28, 28), cmap='gray')\n",
    "    axarr[1].set_title('Gradient Descent w. Bounds')\n",
    "    axarr[2].imshow(lbfgsb_i.reshape(28, 28), cmap='gray')\n",
    "    axarr[2].set_title('L-BFGS-B')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAEORJREFUeJzt3XuwVfV5xvHvo5KJUVIvtAwgw0kpvYhOkRLHJoioTWqYppjScaRtwNEpsUZbq23jZUiI/FEbo9GqdYq34D0MasVRUXum1TqjFkS8gI1SxQo9CsZawUuM8PaPvUi3ePZv77Nvax9+z2fmzNlnvWut/Z495znruvdPEYGZ5Wevshsws3I4/GaZcvjNMuXwm2XK4TfLlMNvlimHfxiTdIGk69o9bwPrCkm/0sLy6yTNbEcv1jz5On9vkHQKcC4wEXgHuBs4PyLeLrOvwUgKYFJEbBik9q/AUUX9tWLa7wDXRURfN/u0NG/5e4Ckc4G/A/4a+AUq4ZkAPCzpUzWW2ad7HQ7Zu8DCspuwNIe/ZJI+C3wXOCsiVkbEzyJiI3AS0Af8STHfIknLJd0i6R3glGLaLVXrmifpVUk/kbRQ0sZiq0v1vJL6il33+ZL+S9Kbki6sWs+Rkh6X9LakAUlX1fonVMPfA3MlTazxO+/e1zJJN0naVhwSTKuad6ykOyVtlfSKpD8fQh+W4PCX7wvAp4G7qidGxHbgfuBLVZNnA8uBA4Bbq+eXdCjwD8AfA2Oo7EGMq/Pc04FfA44Hvi3pN4rpO4C/BEYBv13UzxjC77QZuJbKP7VG/D5wB5XfawVwFYCkvYB7gWeo/C7HA2dL+t0h9GI1OPzlGwW8GREfDVIbKOq7PB4R/xQROyPi/d3m/UPg3oh4LCI+BL4N1Duh892IeD8inqESsN8EiIinIuKJiPio2Av5R+CYIf5efwt8VdLkBuZ9LCLuj4gdwM27+gA+D/xiRFwUER9GxMtU/qmcPMRebBC9fNyYizeBUZL2GeQfwJiivstrifWMra5HxHuSflLnuV+vevwesD+ApF8FLgOmAZ+h8nfyVJ11fUxEbJV0FXARcM0Q+/h0cU5jAjBWUvVJz72BfxtKLzY4b/nL9zjwU+APqidK2h/4CtBfNTm1JR8ADqlafl/g4CZ7ugb4Dypn7D8LXACoifVcAhwL/FaTfbwGvBIRB1R9jYyIWU2uz6o4/CWLiP+lcmx8paQTJI2Q1AcsAzZR2Q1uxHIqu9lfKE7OLaK5wAKMpHK5cbukXwf+rJmVFJcpLwX+psk+/h3YJulbkvaVtLekwyR9vsn1WRWHvwdExPeobF2/TyV0T1LZ6h0fET9tcB3rgLOonDgbALYDW6jsVQzVXwF/BGyjcoz9oybWscsVVE4gDllxDuD3gCnAK1QOga6jcjLTWuSbfPZQxWHD21R23V8pux/rPd7y70EkfVXSZyTtR2Uv4jlgY7ldWa9y+Pcss4H/Lr4mASeHd+2sBu/2m2XKW36zTHX1Jp/i3WBm1kER0dAl3pa2/MV16R9L2iDpvFbWZWbd1fQxv6S9gRepvPFkE7AKmBsR6xPLeMtv1mHd2PIfCWyIiJeLN5LcQeVss5kNA62Efxwff6PJJgZ5C6mkBZJWS1rdwnOZWZt1/IRfRCwBloB3+816SStb/s3A+KqfDymmmdkw0Er4VwGTJH2ueBfZyVQ+hcXMhoGmd/sj4iNJZwIPUvmAhRuKd5aZ2TDQ1dt7fcxv1nlducnHzIYvh98sUw6/WaYcfrNMOfxmmXL4zTLl8JtlyuE3y5TDb5Yph98sUw6/WaYcfrNMOfxmmXL4zTLl8JtlyuE3y5TDb5Yph98sUw6/WaYcfrNMOfxmmXL4zTLl8JtlyuE3y5TDb5Yph98sUw6/WaYcfrNMOfxmmXL4zTK1TysLS9oIbAN2AB9FxLR2NGVmnddS+AvHRsSbbViPmXWRd/vNMtVq+AN4SNJTkhYMNoOkBZJWS1rd4nOZWRspIppfWBoXEZsl/RLwMHBWRDyamL/5JzOzhkSEGpmvpS1/RGwuvm8B7gaObGV9ZtY9TYdf0n6SRu56DHwZeL5djZlZZ7Vytn80cLekXeu5LSJWtqUrM+u4lo75h/xkPuY367iuHPOb2fDl8JtlyuE3y5TDb5Yph98sU+14Y4/1sClTpiTrixcvTtZnzZqVrO+1V3r7sXPnzpq15cuXJ5e98MILk/WBgYFk/dhjj61Z6+/vTy77/vvvJ+t7Am/5zTLl8JtlyuE3y5TDb5Yph98sUw6/WaYcfrNM+Tr/MDBixIhk/ZhjjqlZu/HGG5PLjhkzJlmv967P1HX8esvPmTMnuWy9a+3jx49P1mfOnFmzNn/+/OSyt9xyS7K+J/CW3yxTDr9Zphx+s0w5/GaZcvjNMuXwm2XK4TfLlK/zDwNTp05N1leubP4T0+u9J/7MM89M1t97772mn3vChAnJ+rvvvpusX3nllcn6hx9+WLNW7/fOgbf8Zply+M0y5fCbZcrhN8uUw2+WKYffLFMOv1mmfJ2/B0yePDlZX7FiRdPrrvf59Oeff36yvmbNmqafu56xY8cm6/fcc0+yfsABByTrl1xySc1avdclB3W3/JJukLRF0vNV0w6S9LCkl4rvB3a2TTNrt0Z2+38InLDbtPOA/oiYBPQXP5vZMFI3/BHxKPDWbpNnA0uLx0uBE9vcl5l1WLPH/KMjYtfN0a8Do2vNKGkBsKDJ5zGzDmn5hF9EhKSan9IYEUuAJQCp+cysu5q91PeGpDEAxfct7WvJzLqh2fCvAHZ99vF8IH1Nxsx6Tt3dfkm3AzOBUZI2Ad8BLgaWSToNeBU4qZNN7ukWLlyYrI8aNSpZv++++2rWzjnnnOSyGzZsSNY76bDDDkvWjzjiiJbW38rnHOSgbvgjYm6N0vFt7sXMusi395plyuE3y5TDb5Yph98sUw6/WaZUbwjmtj5Zpnf4XXvttcn6qaeemqzX+wjro446qmZt/fr1yWU7LTW8+EMPPZRcdsaMGcn6I488kqwfd9xxyfqeKiLUyHze8ptlyuE3y5TDb5Yph98sUw6/WaYcfrNMOfxmmfJHd3fBtGnTkvV691ps3749WS/zWn7qOj7A4sWLa9aOPvro5LL1XpeLLrooWbc0b/nNMuXwm2XK4TfLlMNvlimH3yxTDr9Zphx+s0z5Or8l9fX1JetnnHFGsl7vo8NTBgYGkvW1a9c2vW7zlt8sWw6/WaYcfrNMOfxmmXL4zTLl8JtlyuE3y5Sv83dBvffbH3744cn6wQcfnKw//fTTQ+6pUfWGBx87dmyy3sq4EP39/cn622+/3fS6rYEtv6QbJG2R9HzVtEWSNktaW3zN6mybZtZujez2/xA4YZDpP4iIKcXX/e1ty8w6rW74I+JR4K0u9GJmXdTKCb8zJT1bHBYcWGsmSQskrZa0uoXnMrM2azb81wATgSnAAHBprRkjYklETIuI9KdYmllXNRX+iHgjInZExE7gWuDI9rZlZp3WVPgljan68WvA87XmNbPepHrXYSXdDswERgFvAN8pfp4CBLAR+EZEpN98XVlX8xd9h7F99903WV+2bFmyPmtW+kpqK9fSWzV79uxkfd68eTVrc+bMSS47ffr0ZP2JJ55I1nMVEWpkvro3+UTE3EEmXz/kjsysp/j2XrNMOfxmmXL4zTLl8JtlyuE3y1TdS31tfbJML/W1aubMmcl6vSHAU9atW5esP/DAA8n61VdfnayffvrpNWsvvvhictkZM2Yk61u3bk3Wc9XopT5v+c0y5fCbZcrhN8uUw2+WKYffLFMOv1mmHH6zTPk6v7Vkx44dyXrq7+u2225LLpt6O7DV5uv8Zpbk8JtlyuE3y5TDb5Yph98sUw6/WaYcfrNMeYhuS+rr62tp+e3bt9esXX755S2t21rjLb9Zphx+s0w5/GaZcvjNMuXwm2XK4TfLlMNvlqm61/kljQduAkZTGZJ7SURcIekg4EdAH5Vhuk+KiP/pXKtWhoULF7a0/L333luztmbNmpbWba1pZMv/EXBuRBwKHAV8U9KhwHlAf0RMAvqLn81smKgb/ogYiIg1xeNtwAvAOGA2sLSYbSlwYqeaNLP2G9Ixv6Q+4AjgSWB0RAwUpdepHBaY2TDR8L39kvYH7gTOjoh3pP//mLCIiFqfzydpAbCg1UbNrL0a2vJLGkEl+LdGxF3F5DckjSnqY4Atgy0bEUsiYlpEND+apJm1Xd3wq7KJvx54ISIuqyqtAOYXj+cD97S/PTPrlEZ2+78IfB14TtLaYtoFwMXAMkmnAa8CJ3WmReukyZMnJ+tz5sxpaf0PPvhgS8tb59QNf0Q8BtT6HPDj29uOmXWL7/Azy5TDb5Yph98sUw6/WaYcfrNMOfxmmfJHd2du6tSpyfrIkSOT9XpDvH/wwQdD7sm6w1t+s0w5/GaZcvjNMuXwm2XK4TfLlMNvlimH3yxTvs6fuVGjRiXr9a7jr1u3Lllfvnz5kHuy7vCW3yxTDr9Zphx+s0w5/GaZcvjNMuXwm2XK4TfLlK/zZ27evHktLX/zzTe3qRPrNm/5zTLl8JtlyuE3y5TDb5Yph98sUw6/WaYcfrNM1b3OL2k8cBMwGghgSURcIWkR8KfA1mLWCyLi/k41ap2xfv36ZP3www/vUifWbY3c5PMRcG5ErJE0EnhK0sNF7QcR8f3OtWdmnVI3/BExAAwUj7dJegEY1+nGzKyzhnTML6kPOAJ4sph0pqRnJd0g6cAayyyQtFrS6pY6NbO2ajj8kvYH7gTOjoh3gGuAicAUKnsGlw62XEQsiYhpETGtDf2aWZs0FH5JI6gE/9aIuAsgIt6IiB0RsRO4Fjiyc22aWbvVDb8kAdcDL0TEZVXTx1TN9jXg+fa3Z2ad0sjZ/i8CXweek7S2mHYBMFfSFCqX/zYC3+hIh9ZRK1euTNYnTpyYrK9ataqd7VgXNXK2/zFAg5R8Td9sGPMdfmaZcvjNMuXwm2XK4TfLlMNvlimH3yxTqjcEc1ufTOrek5llKiIGuzT/Cd7ym2XK4TfLlMNvlimH3yxTDr9Zphx+s0w5/GaZ6vYQ3W8Cr1b9PKqY1ot6tbde7QvcW7Pa2duERmfs6k0+n3hyaXWvfrZfr/bWq32Be2tWWb15t98sUw6/WabKDv+Skp8/pVd769W+wL01q5TeSj3mN7PylL3lN7OSOPxmmSol/JJOkPRjSRsknVdGD7VI2ijpOUlryx5fsBgDcYuk56umHSTpYUkvFd8HHSOxpN4WSdpcvHZrJc0qqbfxkv5F0npJ6yT9RTG91Ncu0Vcpr1vXj/kl7Q28CHwJ2ASsAuZGRHqg+C6RtBGYFhGl3xAiaQawHbgpIg4rpn0PeCsiLi7+cR4YEd/qkd4WAdvLHra9GE1qTPWw8sCJwCmU+Nol+jqJEl63Mrb8RwIbIuLliPgQuAOYXUIfPS8iHgXe2m3ybGBp8XgplT+erqvRW0+IiIGIWFM83gbsGla+1Ncu0Vcpygj/OOC1qp83UeILMIgAHpL0lKQFZTcziNERMVA8fh0YXWYzg6g7bHs37TasfM+8ds0Md99uPuH3SdMjYirwFeCbxe5tT4rKMVsvXattaNj2bhlkWPmfK/O1a3a4+3YrI/ybgfFVPx9STOsJEbG5+L4FuJveG3r8jV0jJBfft5Tcz8/10rDtgw0rTw+8dr003H0Z4V8FTJL0OUmfAk4GVpTQxydI2q84EYOk/YAv03tDj68A5heP5wP3lNjLx/TKsO21hpWn5Neu54a7j4iufwGzqJzx/0/gwjJ6qNHXLwPPFF/ryu4NuJ3KbuDPqJwbOQ04GOgHXgL+GTioh3q7GXgOeJZK0MaU1Nt0Krv0zwJri69ZZb92ib5Ked18e69ZpnzCzyxTDr9Zphx+s0w5/GaZcvjNMuXwm2XK4TfL1P8BeoxqJ0cm3tMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nine = test_dataset[12][0]\n",
    "fig = plt.figure()\n",
    "plt.imshow(nine.numpy().reshape((28,28)), cmap='gray')\n",
    "plt.title('Original Nine')\n",
    "None # avoid printing title object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints:\n",
    "\n",
    "- You are given a code skeleton with a few functions, but you will probably need to add some more arguments to the functions and their calls.\n",
    "- Split the target \"variable\" i into a fixed part (where it is equal to nine) and a variable part (that is optimized; set requires_grad for this one). Both parts should be tensors and can be combined into another tensor representing i.\n",
    "- Create these two tensors once and then have a function that combines them and calculates the loss.\n",
    "- For the loss it is easiest to implement a function implements the loss translation for the less-or-equal ($\\leq$) and less ($<$) operators from the lecutre. You can express all parts of the loss function with this. Make this parametric in the choice of d.\n",
    "- If implemented correctly your code should not run more than a few seconds.\n",
    "- There is no L-BFGS-B optimizer for pytroch yet. We provide a function that uses scipy to do this instead (see below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[2,1], [3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryNetwork:\n",
    "    \"\"\"\n",
    "    Query a neural network. The constrains are hardcoded in this class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, i_init, NN1_prop, NN2_prop, NN1_logits, NN2_logits, \n",
    "                 init_zero=False, learning_rate = 0.1, **kwargs):\n",
    "        \n",
    "        if init_zero:\n",
    "            i_init = torch.zeros_like(i_init)\n",
    "            \n",
    "        i_temp = i_init.clone()\n",
    "        self.i_fix = i_temp[0, :16, ...] # According to exersice sheet this part shall be fixed\n",
    "        self.i_var = i_temp[0, 16:, ...] # According to exersice sheet this part shall be variable -> requieres grad\n",
    "        self.i_var.requires_grad = True\n",
    "        self.NN1_prop = NN1_prop\n",
    "        self.NN2_prop = NN2_prop\n",
    "        self.NN1_logits = NN1_logits\n",
    "        self.NN2_logits = NN2_logits\n",
    "        self.lbfgsb_NN1 = None\n",
    "        self.lbfgsb_NN2 = None\n",
    "        self.optimizer_sgd = optim.SGD([self.i_var], lr=learning_rate)\n",
    "        self.optimizer_lbfgsb = lbfgsb([self.i_var], 0, 1, self.get_loss(), self.zero_grad())\n",
    "\n",
    "        \n",
    "    def get_i(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Concanate the fixed and the variable part of i\n",
    "        \"\"\"\n",
    "        i = torch.cat((self.i_fix, self.i_var),0)\n",
    "        i = i.reshape((1, 1, 28, 28))\n",
    "        return i\n",
    "\n",
    "    def get_loss(self, use_prob = None, use_logits =  None, add_bounds=False, **kwargs):\n",
    "        \n",
    "        loss = 0\n",
    "        if use_prob is None and use_logits is None:\n",
    "            use_prob=True\n",
    "        \n",
    "        if use_prob and use_logits:\n",
    "            Exception(\"Inconsistent arguments, use_prob and use_logits are both 'True'.\")\n",
    "        \n",
    "        elif use_prob:\n",
    "            NN1_out = self.NN1_prop(self.get_i())\n",
    "            NN2_out = self.NN2_prop(self.get_i())\n",
    "        \n",
    "        elif use_logits:\n",
    "            NN1_out = self.NN1_logits(self.get_i())\n",
    "            NN2_out = self.NN2_logits(self.get_i())\n",
    "            \n",
    "        # Calculate loss for constrain in exercise sheet: class (NN1( i ) ) = 8 \n",
    "        # This is the same as stating that all probabilitiy outputs should be smaller than that of class 8.\n",
    "        # <=> NN1(i)[k] < NN1(8)[k], for all k except 8\n",
    "        loss += self.equal_to_label(NN1_out, 8, **kwargs)\n",
    "        \n",
    "        # Calculate loss for constrain in exercise sheet: class (NN2( i ) ) = 9 \n",
    "        # This is the same as stating that all probabilitiy outputs should be smaller than that of class 8.\n",
    "        # <=> NN2(i)[k] < NN2(9)[k], for all k except 9\n",
    "        loss += self.equal_to_label(NN1_out, 9, **kwargs)\n",
    "        \n",
    "        # Add loss for values which are not in image domain.\n",
    "        if add_bounds:\n",
    "            i_var_flat = self.i_var.view(-1)\n",
    "            for k in range(i_var_flat.size()[0]):\n",
    "                loss += self.lower_equal(0, i_var_flat[k], **kwargs)\n",
    "                loss += self.lower_equal(i_var_flat[k], 1, **kwargs)\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def solve_gd(self,max_iter_=100, learning_rate=None, **kwargs):\n",
    "        #return:\n",
    "        #solved: Bool; did you find a solution\n",
    "        #loss: Float; value of loss at the end    \n",
    "        #i: numpy array; the resulting i\n",
    "        #t: float; how long the execution took\n",
    "        #nr: number of iterations\n",
    "        \n",
    "        t0 = time.time()\n",
    "        loss = 0\n",
    "        solved = False\n",
    "        iter_ = 0\n",
    "        \n",
    "        if learning_rate is not None:\n",
    "            for param_group in self.optimizer_sgd.param_groups:\n",
    "                param_group['lr'] = learning_rate\n",
    "        \n",
    "        while iter_ <= max_iter_:\n",
    "            iter_ += 1\n",
    "            self.optimizer_sgd.zero_grad()   # zero the gradient buffers\n",
    "            loss = self.get_loss(**kwargs)\n",
    "            loss.backward()\n",
    "            self.optimizer_sgd.step()    # Does the update\n",
    "\n",
    "\n",
    "        solved = loss == 0\n",
    "        t1 = time.time()\n",
    "\n",
    "        return solved, loss, self.get_i().detach().numpy(), t1 - t0, iter_\n",
    "    \n",
    "    def solve_lbfgsb(**kwargs):\n",
    "        t0 = time.time()\n",
    "        loss = 0\n",
    "        solved = False\n",
    "        self.lbfgsb_NN1\n",
    "        self.lbfgsb_NN2\n",
    "\n",
    "        #return:\n",
    "        #solved: Bool; did you find a solution\n",
    "        #loss: Float; value of loss at the end    \n",
    "        #i: numpy array; the resulting i\n",
    "        #t: float; how long the execution took\n",
    "        \n",
    "        t1 = time.time()\n",
    "        return solved, loss, nine.detach().numpy(), t1 - t0\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.lbfgsb_NN1.zero_grad()\n",
    "        self.lbfgsb_NN2.zero_grad()\n",
    "        self.i_var.zero_grad()\n",
    "\n",
    "    \n",
    "    def equal_to_label(self,NN_out, equal_label, **kwargs):\n",
    "        loss = 0\n",
    "        out_equal_label= NN_out[0][equal_label]\n",
    "        for label in range(0,NN_out.shape[1]):\n",
    "            if label == equal_label:\n",
    "                continue\n",
    "            loss += self.lower_strictly(NN_out[0][label], out_equal_label, **kwargs)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def lower_strictly(self, t1, t2, p=1, **kwargs):\n",
    "        eps = np.finfo(float).eps\n",
    "        return self.lower_equal(t1 + eps, t2, p=p, **kwargs)\n",
    "\n",
    "    def lower_equal(self, t1, t2, p=1):\n",
    "        if t1 < t2:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return self.distance(t1, t2, p=p)\n",
    "\n",
    "    @staticmethod\n",
    "    def distance(t1, t2, p=1):\n",
    "        return torch.pow(abs(t1 - t2), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feel free to add args to this function\n",
    "def solve_lbfgsb(**kwargs):\n",
    "    t0 = time.time()\n",
    "    t1 = time.time()\n",
    "    loss = 0\n",
    "    solved = False\n",
    "\n",
    "    #Hint:\n",
    "    # Use the provided lbfgsb(var, min_val, max_val, loss_fn, zero_grad_fn)\n",
    "    # function.\n",
    "    # It takes the tensor to optimize (var), the min and max value for each entry (a scalar),\n",
    "    # a function that returns the current loss-tensor and a function that sets the \n",
    "    # gradients of everything used (NN1_logits, NN2_logits) and i_var to zero.\n",
    "    # This funciton does not return anything but changes var.\n",
    "\n",
    "    \n",
    "    #return:\n",
    "    #solved: Bool; did you find a solution\n",
    "    #loss: Float; value of loss at the end    \n",
    "    #i: numpy array; the resulting i\n",
    "    #t: float; how long the execution took\n",
    "    return solved, loss, nine.detach().numpy(), t1 - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using logits, initialized with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'QueryNetwork' object has no attribute 'solve_lbfgsb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-e7726753eb3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m show(qn_init_zero.solve_gd(),\n\u001b[1;32m      5\u001b[0m      \u001b[0mqn_init_zero\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve_gd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_bounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m      \u001b[0mqn_init_zero\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve_lbfgsb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'QueryNetwork' object has no attribute 'solve_lbfgsb'"
     ]
    }
   ],
   "source": [
    "nine = test_dataset[12][0]\n",
    "qn_init_zero = QueryNetwork(nine, NN1, NN2, NN1_logits, NN2_logits, init_zero=True)\n",
    "\n",
    "show(qn_init_zero.solve_gd(),\n",
    "     qn_init_zero.solve_gd(add_bounds=True),\n",
    "     qn_init_zero.solve_lbfgsb()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-8-700e5e1080ec>\u001b[0m(116)\u001b[0;36mdistance\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    112 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0msolved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    113 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    114 \u001b[0;31m    \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    115 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 116 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m^\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> t1 - t2\n",
      "tensor(3.6870e-08, grad_fn=<ThSubBackward>)\n",
      "ipdb> abs(t1 - t2)\n",
      "tensor(3.6870e-08, grad_fn=<AbsBackward>)\n",
      "ipdb> abs(t1 - t2) ^ p\n",
      "*** RuntimeError: bitxor is only supported for integer type tensors at /pytorch/aten/src/TH/generic/THTensorMath.cpp:1225\n",
      "ipdb> abs(t1 - t2)^torch.Tensor(1)\n",
      "*** RuntimeError: cbitxor is only supported for integer type tensors at /pytorch/aten/src/TH/generic/THTensorMath.cpp:1813\n",
      "ipdb> torch.pow(abs(t1 - t2),2(\n",
      "*** SyntaxError: unexpected EOF while parsing\n",
      "ipdb> torch.pow(abs(t1 - t2),2)\n",
      "tensor(1.3594e-15, grad_fn=<PowBackward0>)\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%debug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first image has weird colors as not all values are in [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using logits, initialized with original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(solve_gd(),\n",
    "     solve_gd(add_bounds=True),\n",
    "     solve_lbfgsb())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using probabilites, initialized with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(solve_gd(use_logits=False, init_zero=True),\n",
    "     solve_gd(use_logits=False, init_zero=True),\n",
    "     solve_lbfgsb(use_logits=False, init_zero=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using probabilites, initialized with original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(solve_gd(use_prob=True),\n",
    "     solve_gd(add_bounds=True, use_prob=True),\n",
    "     solve_lbfgsb(use_prob=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that using probabilities is not a viable approach. The numerical optimization problem becomes basically intractable due to the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## different box constraint (task 1.7; optional), using logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(solve_gd(box=2),\n",
    "     solve_gd(add_bounds=True, box=2),\n",
    "     solve_lbfgsb(box=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the region covered by box 2 is mostly emty it does not matter much wether we use init_zero or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
